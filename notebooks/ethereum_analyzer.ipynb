{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62a7822a",
   "metadata": {},
   "source": [
    "# BigQuery Chat with Ethereum\n",
    "\n",
    "This Jupyter notebook allows you to chat with the public Ethereum dataset on BigQuery using natural language. It uses a Gradio UI to provide a user interface for asking questions and viewing the results.\n",
    "\n",
    "## Setup\n",
    "\n",
    "1.  **Install Dependencies:** Follow the readme file of this repo. It will install all the necessary Python libraries using `uv`.\n",
    "2.  **Configure Environment Variables:** Make sure the `.env` file is in the root of this project and add complete variables:\n",
    "    ```\n",
    "    GOOGLE_CLOUD_PROJECT=<your-gcp-project-id>\n",
    "    GOOGLE_CLOUD_LOCATION=<your-gcp-location>\n",
    "    ```\n",
    "3.  **Authentication:** Make sure you are authenticated with Google Cloud. You can do this by running `gcloud auth application-default login` in your terminal.\n",
    "\n",
    "## Execution\n",
    "\n",
    "1.  **Run all cells:** Run all the cells in this notebook to start the Gradio UI.\n",
    "2.  **Ask a question:** Use the textbox to ask a question about the Ethereum dataset (e.g., \"Show the 10 most recent transactions\").\n",
    "3.  **View the results:** The generated SQL query, the results of the query, and any logs will be displayed in the output textboxes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c67d4a",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cc824b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "parent_dir = Path(\"..\").resolve()\n",
    "if str(parent_dir) not in sys.path:\n",
    "\tsys.path.insert(0, str(parent_dir))\n",
    "\tprint(f\"Added {parent_dir} to Python path.\")\n",
    "else:\n",
    "\tprint(f\"{parent_dir} already in Python path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641cef7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set option for Pandas without truncating long strings\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_colwidth\", None)   # don’t truncate long strings\n",
    "pd.set_option(\"display.max_columns\", None)    # show all columns\n",
    "pd.set_option(\"display.width\", 0)             # auto-detect console width"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf77aa4",
   "metadata": {},
   "source": [
    "## BigQuery Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a87bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a BigQuery client using environment variables, with Colab auth if needed.\n",
    "import os, re, sys\n",
    "from dotenv import load_dotenv\n",
    "from google.cloud import bigquery\n",
    "from google.auth.exceptions import DefaultCredentialsError\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def _clean_env(val: str | None, default: str = \"\") -> str:\n",
    "    if val is None:\n",
    "        return default\n",
    "    v = val.strip()\n",
    "    # strip wrapping quotes (either single or double)\n",
    "    if len(v) >= 2 and v[0] == v[-1] and ord(v[0]) in (39, 34):\n",
    "        v = v[1:-1]\n",
    "    # drop inline comments\n",
    "    if \"#\" in v:\n",
    "        v = v.split(\"#\", 1)[0].strip()\n",
    "    return v\n",
    "\n",
    "# Prefer official names, then legacy fallbacks\n",
    "PROJECT_ID = _clean_env(os.getenv(\"GOOGLE_CLOUD_PROJECT\") or os.getenv(\"PROJECT_ID\"), \"your-gcp-project-id\")\n",
    "LOCATION  = _clean_env(os.getenv(\"GOOGLE_CLOUD_LOCATION\") or os.getenv(\"LOCATION\"), \"US\")\n",
    "\n",
    "# Detect Colab and authenticate if necessary\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "if IN_COLAB:\n",
    "    try:\n",
    "        from google.colab import auth as colab_auth  # type: ignore\n",
    "        print(\"Running in Colab: prompting for authentication...\")\n",
    "        colab_auth.authenticate_user()\n",
    "        print(\"Colab authentication completed.\")\n",
    "    except Exception as e:\n",
    "        print(\"Colab auth not available:\", e)\n",
    "\n",
    "# Basic validation for project id\n",
    "if not re.fullmatch(r\"[a-z][a-z0-9-]{4,61}[a-z0-9]\", PROJECT_ID):\n",
    "    print(f\"Warning: PROJECT_ID looks invalid: {PROJECT_ID}\")\n",
    "    print(\"Update your .env, e.g.: GOOGLE_CLOUD_PROJECT=my-gcp-project\")\n",
    "\n",
    "# Tip: Authenticate with Google Cloud before running (choose one):\n",
    "# 1) In Colab: the cell above already handled this.\n",
    "# 2) Locally: gcloud auth application-default login\n",
    "# 3) Or set GOOGLE_APPLICATION_CREDENTIALS to a service account JSON key path\n",
    "\n",
    "try:\n",
    "    client = bigquery.Client(project=PROJECT_ID, location=LOCATION)\n",
    "    print(f\"BigQuery client created for project: {client.project} in {LOCATION}\")\n",
    "except DefaultCredentialsError:\n",
    "    print(\"Google Application Default Credentials not found.\")\n",
    "    print(\"Run: gcloud auth application-default login\")\n",
    "    print(\"Or set GOOGLE_APPLICATION_CREDENTIALS to a service account key JSON file path.\")\n",
    "    client = None\n",
    "except Exception as e:\n",
    "    print(\"Failed to create BigQuery client:\", e)\n",
    "    client = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d391e43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a writable dataset and a remote LLM model using a BigQuery Connection.\n",
    "from google.cloud import bigquery\n",
    "from google.api_core import exceptions as gax_exceptions\n",
    "\n",
    "# Writable dataset in your project (models/tables go here)\n",
    "dataset_id = f\"{PROJECT_ID}.bq_llm\"\n",
    "dataset = bigquery.Dataset(dataset_id)\n",
    "dataset.location = LOCATION\n",
    "try:\n",
    "    client.create_dataset(dataset, exists_ok=True)\n",
    "    print(f\"Dataset ensured: {dataset_id} ({LOCATION})\")\n",
    "except Exception as e:\n",
    "    print(\"Failed to create/ensure dataset:\", e)\n",
    "\n",
    "# Connection resource (edit name if your connection differs)\n",
    "connection_name = \"bq-llm-connection\"\n",
    "connection_resource = f\"projects/{PROJECT_ID}/locations/{LOCATION}/connections/{connection_name}\"\n",
    "print(\"Using connection:\", connection_resource)\n",
    "\n",
    "# Heads-up: public dataset bigquery-public-data.crypto_ethereum.* is in US region.\n",
    "if LOCATION.upper() != \"US\":\n",
    "    print(\"Warning: LOCATION is not US. Reading US public data with a non-US model/connection may cause cross-region errors.\")\n",
    "    print(\"Set GOOGLE_CLOUD_LOCATION=US in your .env or copy data into a dataset in your region.\")\n",
    "\n",
    "# Create (or replace) the remote LLM model in your project\n",
    "summarizer_query = f\"\"\"\n",
    "CREATE OR REPLACE MODEL `{dataset_id}.transaction_summarizer`\n",
    "REMOTE WITH CONNECTION `{connection_resource}`\n",
    "OPTIONS (\n",
    "  remote_service_type = 'CLOUD_AI_LARGE_LANGUAGE_MODEL_V1',\n",
    "  endpoint = 'gemini-2.5-pro'\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "model_ready = False\n",
    "try:\n",
    "    client.query(summarizer_query).result()\n",
    "    print(\"Remote LLM model ensured:\", f\"{dataset_id}.transaction_summarizer\")\n",
    "    model_ready = True\n",
    "except gax_exceptions.NotFound as e:\n",
    "    print(\"Connection not found:\", connection_resource)\n",
    "    print(\"Tip: Create a BigQuery connection named 'bq-llm-connection' in this location and grant it permissions.\")\n",
    "except gax_exceptions.Forbidden as e:\n",
    "    print(\"Permission error when creating model. Ensure you have BigQuery Admin and connection access.\")\n",
    "    print(str(e))\n",
    "except Exception as e:\n",
    "    print(\"Failed to create remote model:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73921e8b",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "- **`DefaultCredentialsError`**: In Colab, re-run the **Auth** cell. Locally, run `gcloud auth application-default login` or set `GOOGLE_APPLICATION_CREDENTIALS` to a service-account JSON.\n",
    "- **`NotFound: Connection not found`**: Create a BigQuery **Connection** named `bq-llm-connection` in the `GOOGLE_CLOUD_LOCATION` region, and grant the notebook's identity permission to use it.\n",
    "- **Cross-region errors**: The public dataset `bigquery-public-data.crypto_ethereum.*` is in the **US** multi-region. Set `GOOGLE_CLOUD_LOCATION=US` (or stage the data to your region).\n",
    "- **Permission denied**: Ensure your identity has BigQuery Admin (or sufficient) permissions **and** connection usage rights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e23237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional cleanup: set CLEANUP=True to drop the model and/or dataset\n",
    "CLEANUP = False\n",
    "\n",
    "if CLEANUP and client is not None:\n",
    "    try:\n",
    "        client.delete_model(f\"{dataset_id}.transaction_summarizer\", not_found_ok=True)\n",
    "        print(\"Deleted model:\", f\"{dataset_id}.transaction_summarizer\")\n",
    "    except Exception as e:\n",
    "        print(\"Model delete failed:\", e)\n",
    "    try:\n",
    "        client.delete_dataset(dataset_id, delete_contents=True, not_found_ok=True)\n",
    "        print(\"Deleted dataset:\", dataset_id)\n",
    "    except Exception as e:\n",
    "        print(\"Dataset delete failed:\", e)\n",
    "else:\n",
    "    print(\"Cleanup skipped.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfacfae",
   "metadata": {},
   "source": [
    "## The AI Architect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846657c1",
   "metadata": {},
   "source": [
    "- Purpose: Translate natural language into safe, parameterized SQL over BigQuery’s public Ethereum dataset, execute it, and explain results.\n",
    "- Config: project `{PROJECT_ID}`, location `{LOCATION}`.\n",
    "- Remote LLM model: `{dataset_id}.transaction_summarizer` (gemini-2.5-pro) via `{connection_resource}` — ready: {model_ready}.\n",
    "- What you get:\n",
    "\t- Generated SQL preview\n",
    "\t- Query results table\n",
    "\t- Optional LLM summary of results\n",
    "\t- Execution logs/errors\n",
    "- Safety:\n",
    "\t- Read-only queries against `bigquery-public-data.crypto_ethereum.*`\n",
    "\t- Auto LIMITs for large scans (tunable in UI)\n",
    "\t- Note: public dataset is in US; cross-region reads can error if LOCATION != US\n",
    "- How to use:\n",
    "\t1) Run the UI cell with `phase = \"architect\"`.\n",
    "\t2) Ask a question (e.g., “Show the 10 most recent transactions”).\n",
    "- Example prompts:\n",
    "\t- \"List the latest 10 transactions.\"\n",
    "\t- \"Average gas price by day over the past week.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88f50b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the UI with phase selection\n",
    "from bigq_ai_app.ui import create_ui\n",
    "\n",
    "# Choose phase: architect, semantic, multimodal\n",
    "phase = \"architect\" \n",
    "\n",
    "demo = create_ui(phase)\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe617857",
   "metadata": {},
   "source": [
    "## The AI Semantic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6417190",
   "metadata": {},
   "source": [
    "- Purpose: Semantic retrieval over your ingested docs (RAG). It embeds queries with `{model_fqn}` and finds nearest chunks in `{table_fqn}` using BigQuery VECTOR_SEARCH, then optionally summarizes.\n",
    "- Config: project `{project_id}`, location `{location}`.\n",
    "- Data/Models:\n",
    "\t- Docs table: `{table_fqn}` (columns like source_file, kind, content, embedding)\n",
    "\t- Embedding model: `{model_fqn}` (endpoint `text-embedding-004` via `{conn_res}`)\n",
    "\t- Optional vector index: `semantic_docs_index` on the embedding column\n",
    "- What you get:\n",
    "\t- Top-k matches with content previews and distances\n",
    "\t- (Optional) LLM summary grounded on retrieved chunks\n",
    "\t- Execution logs/errors\n",
    "- Safety:\n",
    "\t- Read-only queries against your docs table\n",
    "\t- Uses brute-force search by default for small tables; switch to index when created\n",
    "- How to use:\n",
    "\t1) Ingest docs (run the ingestion cells to populate `{table_fqn}` and embeddings).\n",
    "\t2) Optionally create the vector index (see the index DDL cell).\n",
    "\t3) Launch the UI with `phase = \"semantic\"` (see the UI cell).\n",
    "\t4) Ask a question (e.g., “What is the Ethereum data model?”).\n",
    "- Example prompts:\n",
    "\t- \"What is the Ethereum data model?\"\n",
    "\t- \"Where are ERC-20 transfers stored?\"\n",
    "\t- \"Summarize glossary definitions for gas and gas price.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb1d55c",
   "metadata": {},
   "source": [
    "### Docs ingestion and embeddings (Semantic setup)\n",
    "\n",
    "First we prepare the semantic search dataset:\n",
    "- Ingest docs from ../docs into table [PROJECT_ID].bq_llm.semantic_docs using PROSE_CHUNK and SQL_CHUNK. Each type chunk has different length.\n",
    "- PROSE_CHUNK: Chunking(max_chars=1100, overlap=180)\n",
    "- SQL_CHUNK: Chunking(max_chars=600, overlap=80)\n",
    "- Controlled by RESET_TABLE_ONCE and DRY_RUN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cb5cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigq_ai_app.utils.ingestion import (\n",
    "    ingest_docs_folder,# create_embedding_model, upsert_embeddings_for_new_rows,\n",
    "    PROSE_CHUNK, SQL_CHUNK\n",
    ")\n",
    "\n",
    "DOCS_DIR   = Path(\"..\") / \"docs\"   # notebook sits in notebooks/\n",
    "DATASET_ID = \"bq_llm\"              # plain dataset name only\n",
    "TABLE_NAME = \"semantic_docs\"\n",
    "\n",
    "# --- Run 1: initialize the table schema (DROP + CREATE), then ingest ---\n",
    "RESET_TABLE_ONCE = True     # only on the very first run\n",
    "DRY_RUN = False             # set to False to actually write\n",
    "\n",
    "ingest_docs_folder(\n",
    "    client=client,\n",
    "    project_id=PROJECT_ID,\n",
    "    location=LOCATION,\n",
    "    docs_dir=DOCS_DIR,\n",
    "    dataset_id=DATASET_ID,\n",
    "    table_name=TABLE_NAME,\n",
    "    dry_run=DRY_RUN,\n",
    "    filter_filenames=[\"Sample-SQL-queries.md\",\"Ethereum-blockchain-data-model.md\",\"Ethereum-business-glossary.md\"],\n",
    "    prose_chunk=PROSE_CHUNK,\n",
    "    sql_chunk=SQL_CHUNK,\n",
    "    reset_table_once=RESET_TABLE_ONCE,     # First run does DROP+CREATE\n",
    "    truncate_before_insert=False           # (ignored because we reset)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdee738",
   "metadata": {},
   "source": [
    "Simple query to verify the content length of each chunk. Here is where the fine tuning will occur depending of the type of documents to ingest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34734d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "if client is None:\n",
    "    raise RuntimeError(\"BigQuery client is not initialized. Fix auth/credentials above and re-run.\")\n",
    "\n",
    "TEST_SQL = \"\"\"\n",
    "SELECT\n",
    "  id,\n",
    "  chunk_index,\n",
    "  CHAR_LENGTH(content) AS content_length\n",
    "FROM\n",
    "  `gen-lang-client-0962868402.bq_llm.semantic_docs`\n",
    "ORDER BY\n",
    "  content_length DESC\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "df_test = client.query(TEST_SQL).to_dataframe()\n",
    "df_test\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824067e0",
   "metadata": {},
   "source": [
    " Now we keep the schema, but make sure to truncate content before inserting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227cc60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigq_ai_app.utils.ingestion import (\n",
    "    #ingest_docs_folder, \n",
    "    create_embedding_model, upsert_embeddings_for_new_rows,\n",
    "    debug_embedding_row_by_row, #new import for debugging\n",
    "    PROSE_CHUNK, SQL_CHUNK\n",
    ")\n",
    "\n",
    "DOCS_DIR   = Path(\"..\") / \"docs\"   # notebook sits in notebooks/\n",
    "DATASET_ID = \"bq_llm\"              # plain dataset name only\n",
    "TABLE_NAME = \"semantic_docs\"\n",
    "\n",
    "# --- Runs new 2+ :---\n",
    "RESET_TABLE_ONCE = False\n",
    "DRY_RUN = False\n",
    "\n",
    "import time\n",
    "\n",
    "# --- This is your existing ingestion code ---\n",
    "ingest_docs_folder(\n",
    "    client=client,\n",
    "    project_id=PROJECT_ID,\n",
    "    location=LOCATION,\n",
    "    docs_dir=DOCS_DIR,\n",
    "    dataset_id=DATASET_ID,\n",
    "    table_name=TABLE_NAME,\n",
    "    dry_run=False,\n",
    "    reset_table_once=False,\n",
    "    truncate_before_insert=True\n",
    ")\n",
    "\n",
    "# --- This is your existing embedding code ---\n",
    "MODEL_PATH = create_embedding_model(\n",
    "    client, PROJECT_ID, LOCATION, DATASET_ID,\n",
    "    model_name=\"text_embedding_model\",\n",
    "    connection_name=\"bq-llm-connection\",\n",
    "    endpoint=\"text-embedding-004\"\n",
    ")\n",
    "\n",
    "# This should now work without the streaming buffer error\n",
    "upsert_embeddings_for_new_rows(client, PROJECT_ID, LOCATION, DATASET_ID, TABLE_NAME, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1fd6e5",
   "metadata": {},
   "source": [
    "Preview rows that already have embeddings in the semantic_docs table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1115566",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_fqn = f\"{PROJECT_ID}.{DATASET_ID}.{TABLE_NAME}\"\n",
    "\n",
    "# Try to detect the embedding column from the table schema\n",
    "t = client.get_table(table_fqn)\n",
    "embed_col = None\n",
    "for f in t.schema:\n",
    "\tif f.mode == \"REPEATED\" and f.field_type.upper() in (\"FLOAT\", \"FLOAT64\"):\n",
    "\t\tembed_col = f.name\n",
    "\t\tbreak\n",
    "# Fallback to common names if not found via schema\n",
    "if embed_col is None:\n",
    "\tfor cand in (\"embedding\", \"embeddings\", \"vector\", \"embedding_vec\"):\n",
    "\t\tif any(sf.name == cand for sf in t.schema):\n",
    "\t\t\tembed_col = cand\n",
    "\t\t\tbreak\n",
    "\n",
    "if embed_col is None:\n",
    "\tprint(\"No embedding-like column detected. Table schema:\")\n",
    "\tfor f in t.schema:\n",
    "\t\tprint(f\"  - {f.name}: {f.field_type} (mode={f.mode})\")\n",
    "else:\n",
    "\tsql = f\"\"\"\n",
    "\tSELECT\n",
    "\t  id,\n",
    "\t  chunk_index,\n",
    "\t  SUBSTR(content, 1, 160) AS content_preview,\n",
    "\t  ARRAY_LENGTH({embed_col}) AS embedding_dim,\n",
    "\t  {embed_col} AS embedding\n",
    "\tFROM `{table_fqn}`\n",
    "\tWHERE {embed_col} IS NOT NULL\n",
    "\tORDER BY chunk_index\n",
    "\tLIMIT 8\n",
    "\t\"\"\"\n",
    "\tdf_embed = client.query(sql).to_dataframe()\n",
    "\n",
    "\t# Compact preview of the embedding values (first few numbers)\n",
    "\tdef _head(x, n=8):\n",
    "\t\ttry:\n",
    "\t\t\treturn list(x[:n])\n",
    "\t\texcept Exception:\n",
    "\t\t\ttry:\n",
    "\t\t\t\timport numpy as np  # already available in env typically; used only if present\n",
    "\t\t\t\treturn list(np.array(x).flatten()[:n])\n",
    "\t\t\texcept Exception:\n",
    "\t\t\t\treturn None\n",
    "\n",
    "\tif df_embed.empty:\n",
    "\t\tcounts = client.query(\n",
    "\t\t\tf\"SELECT COUNTIF({embed_col} IS NOT NULL) AS with_embed, COUNT(*) AS total FROM `{table_fqn}`\"\n",
    "\t\t).to_dataframe().iloc[0]\n",
    "\t\tprint(f\"No rows with embeddings yet. with_embed={counts['with_embed']}, total={counts['total']}\")\n",
    "\telse:\n",
    "\t\tdf_embed[\"embedding_head\"] = df_embed[\"embedding\"].apply(_head)\n",
    "\t\tpreview_cols = [\"id\", \"chunk_index\", \"content_preview\", \"embedding_dim\", \"embedding_head\"]\n",
    "\t\tdisplay(df_embed[preview_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8460e414",
   "metadata": {},
   "source": [
    "Vector Index will fail as there are less than 500 records in our vector database. So the error is captured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ae3f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safely create the vector index with error handling (no raise)\n",
    "index_name = \"semantic_docs_index\"\n",
    "\n",
    "if client is None:\n",
    "\tprint(\"BigQuery client is not initialized. Skipping vector index creation.\")\n",
    "elif not table_fqn or not embed_col:\n",
    "\tprint(\"Table FQN or embedding column not set. Skipping vector index creation.\")\n",
    "else:\n",
    "\tddl = f\"\"\"\n",
    "\tCREATE OR REPLACE VECTOR INDEX {index_name}\n",
    "\tON `{table_fqn}`({embed_col})\n",
    "\tOPTIONS(index_type = 'IVF', distance_type = 'COSINE');\n",
    "\t\"\"\"\n",
    "\ttry:\n",
    "\t\tjob = client.query(ddl)\n",
    "\t\tjob.result()\n",
    "\t\tprint(f\"Vector index ensured: {index_name} on {table_fqn} (distance=COSINE, type=IVF)\")\n",
    "\texcept Exception as e:\n",
    "\t\tprint(f\"Vector index creation skipped due to error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac38308c",
   "metadata": {},
   "source": [
    "### Retreival\n",
    "\n",
    "Now let's retrieve some data by asking \"What is the Ethereum data model?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa9df91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- For small tables (< 5,000 rows), you don't need to create an index.\n",
    "#-- Instead, use the \"use_brute_force\" option in VECTOR_SEARCH.\n",
    "\n",
    "# Initialize the BigQuery client. This object will send the query to the API.\n",
    "client = bigquery.Client()\n",
    "\n",
    "\n",
    "# 1. Define search parameters\n",
    "search_query = \"What is the Ethereum data model?\"\n",
    "table_fqn_str = f\"{PROJECT_ID}.bq_llm.semantic_docs\"\n",
    "model_path = f\"`{PROJECT_ID}.bq_llm.text_embedding_model`\"\n",
    "embed_col = None  # Will be detected\n",
    "top_k = 5\n",
    "\n",
    "# Detect the embedding column from the table schema\n",
    "t = client.get_table(table_fqn_str)\n",
    "print(f\"Table schema for {table_fqn_str}:\")\n",
    "for f in t.schema:\n",
    "    print(f\"  - {f.name}: {f.field_type} (mode={f.mode})\")\n",
    "schema_names = [f.name for f in t.schema]\n",
    "for f in t.schema:\n",
    "    if f.mode == \"REPEATED\" and f.field_type.upper() in (\"FLOAT\", \"FLOAT64\"):\n",
    "        embed_col = f.name\n",
    "        break\n",
    "# Fallback to common names if not found via schema\n",
    "if embed_col is None:\n",
    "    for cand in (\"embedding\", \"embeddings\", \"vector\", \"embedding_vec\"):\n",
    "        if cand in schema_names:\n",
    "            f = next((f for f in t.schema if f.name == cand), None)\n",
    "            if f and f.mode == \"REPEATED\" and f.field_type.upper() in (\"FLOAT\", \"FLOAT64\"):\n",
    "                embed_col = cand\n",
    "                break\n",
    "\n",
    "if embed_col is None:\n",
    "    print(\"No embedding column found.\")\n",
    "    raise ValueError(\"No embedding column found in the table schema.\")\n",
    "\n",
    "print(f\"Using embedding column: {embed_col}\")\n",
    "\n",
    "# Prepare column name literals (VECTOR_SEARCH expects column names as strings)\n",
    "embed_col_literal = f\"'{embed_col}'\"\n",
    "query_col_literal = \"'ml_generate_embedding_result'\"\n",
    "\n",
    "# 2. Construct the vector search query\n",
    "sql_query = f\"\"\"\n",
    "WITH query_table AS (\n",
    "  -- Generate an embedding for our search query on the fly.\n",
    "  SELECT ml_generate_embedding_result\n",
    "  FROM\n",
    "    ML.GENERATE_EMBEDDING(\n",
    "      MODEL {model_path},\n",
    "      (SELECT '{search_query}' AS content)\n",
    "    )\n",
    ")\n",
    "-- Find the nearest neighbors using VECTOR_SEARCH.\n",
    "SELECT\n",
    "  search_result.base.source_file AS source_file,\n",
    "  search_result.base.content AS content,\n",
    "  search_result.distance AS distance\n",
    "FROM\n",
    "  VECTOR_SEARCH(\n",
    "    TABLE `{table_fqn_str}`,\n",
    "    {embed_col_literal},\n",
    "    TABLE query_table,\n",
    "    {query_col_literal},\n",
    "    top_k => {top_k},\n",
    "    distance_type => 'COSINE',\n",
    "    options => '{{\"use_brute_force\": true}}'\n",
    "  ) AS search_result\n",
    "ORDER BY\n",
    "  distance\n",
    "\"\"\"\n",
    "\n",
    "# 3. Execute the query and display the results\n",
    "if client is None:\n",
    "    raise RuntimeError(\"BigQuery client is not initialized.\")\n",
    "\n",
    "print(f\"Performing vector search for: '{search_query}'...\")\n",
    "job = client.query(sql_query)\n",
    "results_df = job.to_dataframe()  # Waits for completion and returns a DataFrame\n",
    "\n",
    "print(\"Search complete. Top results:\")\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736f2230",
   "metadata": {},
   "source": [
    "Vector search with optional pre-filter on base_table. You can filter by columns like kind or source_file before searching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d868e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 0) Ensure client exists\n",
    "if client is None:\n",
    "    raise RuntimeError(\"BigQuery client is not initialized.\")\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# 1) Parameters\n",
    "search_query_pf = \"What is the Ethereum data model?\"  # your query text\n",
    "project_id = PROJECT_ID\n",
    "dataset_id = DATASET_ID if 'DATASET_ID' in globals() else 'bq_llm'\n",
    "table_name = TABLE_NAME if 'TABLE_NAME' in globals() else 'semantic_docs'\n",
    "\n",
    "# Set one or both filters below (leave as None/empty for no filtering)\n",
    "filter_kind: str | None = None            # e.g., \"glossary\" or \"model\"\n",
    "filter_source_contains: str | None = None # e.g., \"Ethereum-blockchain-data-model.md\"\n",
    "\n",
    "# Use brute force for small tables; set to False to allow index usage if present\n",
    "use_brute_force = True\n",
    "\n",
    "# 2) Resolve FQN and detect embedding column\n",
    "table_fqn_str = f\"{project_id}.{dataset_id}.{table_name}\"\n",
    "model_path = f\"`{project_id}.{dataset_id}.text_embedding_model`\"\n",
    "\n",
    "# Detect embedding column (ARRAY<FLOAT64>)\n",
    "t = client.get_table(table_fqn_str)\n",
    "embed_col = None\n",
    "for f in t.schema:\n",
    "    if f.mode == \"REPEATED\" and f.field_type.upper() in (\"FLOAT\", \"FLOAT64\"):\n",
    "        embed_col = f.name\n",
    "        break\n",
    "if embed_col is None:\n",
    "    # Fallback common names\n",
    "    for cand in (\"embedding\", \"embeddings\", \"vector\", \"embedding_vec\"):\n",
    "        f = next((sf for sf in t.schema if sf.name == cand), None)\n",
    "        if f and f.mode == \"REPEATED\" and f.field_type.upper() in (\"FLOAT\", \"FLOAT64\"):\n",
    "            embed_col = cand\n",
    "            break\n",
    "if embed_col is None:\n",
    "    raise ValueError(\"No embedding column (ARRAY<FLOAT64>) found in table schema.\")\n",
    "\n",
    "print(f\"Using embedding column: {embed_col}\")\n",
    "\n",
    "# 3) Build WHERE clause safely\n",
    "def _sql_quote(s: str) -> str:\n",
    "    return s.replace(\"'\", \"''\")\n",
    "\n",
    "where_parts: list[str] = []\n",
    "if filter_kind:\n",
    "    where_parts.append(f\"kind = '{_sql_quote(filter_kind)}'\")\n",
    "if filter_source_contains:\n",
    "    where_parts.append(f\"STRPOS(source_file, '{_sql_quote(filter_source_contains)}') > 0\")\n",
    "\n",
    "where_clause = (\" WHERE \" + \" AND \".join(where_parts)) if where_parts else \"\"\n",
    "\n",
    "# 4) Prepare VECTOR_SEARCH literals\n",
    "embed_col_literal = f\"'{embed_col}'\"\n",
    "query_col_literal = \"'ml_generate_embedding_result'\"\n",
    "options_sql_literal = \"'{\" + (\"\\\"use_brute_force\\\": true\" if use_brute_force else \"\") + \"}'\"\n",
    "# If not using brute force, '{}' is acceptable; above builds '{}' when false.\n",
    "\n",
    "# 5) Construct and run the query using base_table_query with optional WHERE\n",
    "sql_query_pf = f\"\"\"\n",
    "WITH query_table AS (\n",
    "  SELECT ml_generate_embedding_result\n",
    "  FROM ML.GENERATE_EMBEDDING(\n",
    "    MODEL {model_path},\n",
    "    (SELECT '{_sql_quote(search_query_pf)}' AS content)\n",
    "  )\n",
    ")\n",
    "SELECT\n",
    "  search_result.base.source_file AS source_file,\n",
    "  search_result.base.kind AS kind,\n",
    "  search_result.base.chunk_index AS chunk_index,\n",
    "  SUBSTR(search_result.base.content, 1, 240) AS content_preview,\n",
    "  search_result.distance AS distance\n",
    "FROM\n",
    "  VECTOR_SEARCH(\n",
    "    (SELECT * FROM `{table_fqn_str}`{where_clause}),\n",
    "    {embed_col_literal},\n",
    "    TABLE query_table,\n",
    "    {query_col_literal},\n",
    "    top_k => 5,\n",
    "    distance_type => 'COSINE',\n",
    "    options => {options_sql_literal}\n",
    "  ) AS search_result\n",
    "ORDER BY distance\n",
    "\"\"\"\n",
    "\n",
    "print(\"Pre-filter WHERE clause:\", where_clause or \"<none>\")\n",
    "print(f\"Performing vector search with pre-filter for: '{search_query_pf}'...\")\n",
    "job_pf = client.query(sql_query_pf)\n",
    "results_pf = job_pf.to_dataframe()\n",
    "print(\"Search complete. Top (pre-filtered) results:\")\n",
    "display(results_pf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146bd95c",
   "metadata": {},
   "source": [
    "Let's see the UI and ask for example: What is the Ethereum data model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96238ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the UI with phase selection\n",
    "from bigq_ai_app.ui import create_ui\n",
    "\n",
    "# Choose phase: architect, semantic, multimodal\n",
    "phase = \"semantic\"  # change to \"semantic\" or \"multimodal\"\n",
    "\n",
    "demo = create_ui(phase)\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0e7d1e",
   "metadata": {},
   "source": [
    "## The Multimodal Pioneer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e69381",
   "metadata": {},
   "source": [
    "### OCR PDFs with Vision + BigFrames\n",
    "\n",
    "This section uses Google Vision OCR and handles uploaded PDFs.\n",
    "- If a file is uploaded, it extracts text (Vision OCR via GCS for PDFs; local fallback for non-cloud).\n",
    "- If no file is uploaded, it falls back to the semantic retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3da5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install optional libs if needed (uncomment if missing)\n",
    "# import sys\n",
    "# !{sys.executable} -m uv pip install google-cloud-vision google-cloud-storage bigframes pdfplumber\n",
    "\n",
    "from bigq_ai_app.core.multimodal_orchestrator import extract_text_from_files\n",
    "from bigq_ai_app.core.config import BaseConfig\n",
    "\n",
    "# Example usage with a local test file (replace with your path)\n",
    "local_txt = \"../tests/test_document.txt\"\n",
    "texts, notes, uris = extract_text_from_files([local_txt])\n",
    "print(\"Notes:\\n\", notes)\n",
    "print(\"Extracted text head:\\n\", (texts[0] or \"\")[:200])\n",
    "\n",
    "# If you have a GCS bucket configured and a pdf in local path, upload+OCR\n",
    "# BaseConfig.GCS_BUCKET should be set; e.g., in .env: GCS_BUCKET=my-bucket\n",
    "# local_pdf = \"../docs/sample.pdf\"\n",
    "# texts, notes, uris = extract_text_from_files([local_pdf])\n",
    "# print(\"Vision OCR notes:\\n\", notes)\n",
    "# print(\"GCS URIs:\", uris)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefc278d",
   "metadata": {},
   "source": [
    "For testing the Gradio app, you can use the image file at `../tests/original-ethereum-whitepaper-page1.png`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd684f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the UI with phase selection\n",
    "from bigq_ai_app.ui import create_ui\n",
    "\n",
    "# Choose phase: architect, semantic, multimodal\n",
    "phase = \"multimodal\"\n",
    "\n",
    "demo = create_ui(phase)\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516acbc5",
   "metadata": {},
   "source": [
    "Thank you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigq-kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
