{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BigQuery Remote LLM API Test\n",
    "\n",
    "**Goal:** This notebook provides a comprehensive test for using Google BigQuery to call a remote Large Language Model (like Gemini) using the `ML.GENERATE_TEXT` function.\n",
    "\n",
    "**What it does:**\n",
    "1.  Checks dependencies and authenticates with Google Cloud.\n",
    "2.  Initializes a BigQuery client.\n",
    "3.  Creates a BigQuery dataset to house a remote model.\n",
    "4.  Creates a remote model that connects to a Vertex AI LLM endpoint (`gemini-1.0-pro`).\n",
    "5.  Queries a public Ethereum dataset and passes transaction data to the LLM for summarization.\n",
    "6.  Displays the generated summary.\n",
    "7.  Provides an optional cleanup step to delete the created resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pre-computation Setup\n",
    "\n",
    "Before running this notebook, you must perform a few one-time setup steps in your Google Cloud project.\n",
    "\n",
    "### Required APIs\n",
    "Ensure the following APIs are **enabled** in your project:\n",
    "1.  **BigQuery API**: `gcloud services enable bigquery.googleapis.com`\n",
    "2.  **BigQuery Connection API**: `gcloud services enable bigqueryconnection.googleapis.com`\n",
    "3.  **Vertex AI API**: `gcloud services enable aiplatform.googleapis.com`\n",
    "\n",
    "### BigQuery Connection\n",
    "You need a `CLOUD_RESOURCE` connection in BigQuery that allows BigQuery to communicate with Vertex AI. \n",
    "\n",
    "1.  **Create the connection** (using the gcloud CLI):\n",
    "    ```bash\n",
    "    gcloud bq connections create bq-llm-connection \\\n",
    "        --location=US \\\n",
    "        --project_id=\"YOUR_PROJECT_ID\" \\\n",
    "        --connection_type=CLOUD_RESOURCE\n",
    "    ```\n",
    "2.  **Get the Service Account**: After creating the connection, retrieve the service account associated with it.\n",
    "    ```bash\n",
    "    gcloud bq connections describe bq-llm-connection --location=US --project_id=\"YOUR_PROJECT_ID\"\n",
    "    ```\n",
    "    Look for the `serviceAccountId` in the output (e.g., `bqcx-1234-...@gcp-sa-bigquery-condel.iam.gserviceaccount.com`).\n",
    "\n",
    "3.  **Grant Permissions**: Grant the `Vertex AI User` role to this service account so it can invoke the LLM.\n",
    "    ```bash\n",
    "    gcloud projects add-iam-policy-binding YOUR_PROJECT_ID \\\n",
    "        --member='serviceAccount:YOUR_CONNECTION_SERVICE_ACCOUNT' \\\n",
    "        --role='roles/aiplatform.user'\n",
    "    ```\n",
    "\n",
    "### Authentication\n",
    "* **In Colab**: The first code cell will prompt you to authenticate with your Google account.\n",
    "* **In a local Jupyter environment**: Authenticate via the gcloud CLI before starting the notebook:\n",
    "    ```bash\n",
    "    gcloud auth application-default login\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set your Google Cloud project details here. The `LOCATION` is set to `US` because the public Ethereum dataset we are querying resides in the US multi-region. Cross-region queries between a model and source data are not allowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "parent_dir = Path(\"..\").resolve()\n",
    "if str(parent_dir) not in sys.path:\n",
    "\tsys.path.insert(0, str(parent_dir))\n",
    "\tprint(f\"Added {parent_dir} to Python path.\")\n",
    "else:\n",
    "\tprint(f\"{parent_dir} already in Python path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "forms": {
      "H-vK1Q6m72iL": true
     }
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "BQ_CONNECTION_ID = \"bq-llm-connection\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dependency Checks\n",
    "This cell ensures that the required Python libraries are installed and prints their versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: kernel, packages, and optional .env\n",
    "print(f\"Python: {sys.version.split()[0]}\")\n",
    "\n",
    "try:\n",
    "    import jupyter, ipykernel  # noqa: F401\n",
    "    print(\"Jupyter + ipykernel: OK\")\n",
    "except Exception as e:\n",
    "    print(\"Jupyter import failed:\", e)\n",
    "\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    # In a local environment, you can use a .env file for credentials\n",
    "    if load_dotenv():\n",
    "        print(\"python-dotenv: loaded .env\")\n",
    "except ImportError:\n",
    "    print(\"python-dotenv not installed, skipping .env load.\")\n",
    "    \n",
    "# Verify BigQuery + DataFrame stack\n",
    "try:\n",
    "    import google.cloud.bigquery as bq\n",
    "    import pandas as pd\n",
    "    import pyarrow as pa\n",
    "    import google.cloud.bigquery_storage as bqstorage\n",
    "\n",
    "    print(\"-\" * 20)\n",
    "    print(\"Required packages:\")\n",
    "    print(\"google-cloud-bigquery:\", bq.__version__)\n",
    "    print(\"pandas:\", pd.__version__)\n",
    "    print(\"pyarrow:\", pa.__version__)\n",
    "    print(\"bigquery-storage:\", bqstorage.__version__)\n",
    "except ImportError as e:\n",
    "    print(f\"Missing a required package: {e.name}. Please install it.\")\n",
    "    print(\"pip install google-cloud-bigquery pandas pyarrow db-dtypes google-cloud-bigquery-storage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize BigQuery Client\n",
    "\n",
    "Now we create the BigQuery client object. This will use the credentials configured in the previous steps to interact with your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import re\n",
    "from google.auth.exceptions import DefaultCredentialsError\n",
    "import os\n",
    "\n",
    "client = None\n",
    "PROJECT_ID = os.getenv(\"GOOGLE_CLOUD_PROJECT\")\n",
    "LOCATION = os.getenv(\"GOOGLE_CLOUD_LOCATION\")\n",
    "\n",
    "# Basic validation for project id\n",
    "if not PROJECT_ID or not re.fullmatch(r\"[a-z][a-z0-9-]{4,61}[a-z0-9]\", PROJECT_ID):\n",
    "    print(f\"Error: Project ID is missing or looks invalid: '{PROJECT_ID}'\")\n",
    "    print(\"Please set the GOOGLE_CLOUD_PROJECT variable in the configuration cell above.\")\n",
    "else:\n",
    "    try:\n",
    "        client = bigquery.Client(project=PROJECT_ID, location=LOCATION)\n",
    "        print(f\"BigQuery client created for project: {client.project} in location: {client.location}\")\n",
    "    except DefaultCredentialsError:\n",
    "        print(\"Authentication failed: Google Application Default Credentials not found.\")\n",
    "        print(\"Please complete the authentication steps in section 1.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create BigQuery client: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Remote LLM Model in BigQuery\n",
    "\n",
    "This is the core of the setup. We execute a `CREATE MODEL` query in BigQuery.\n",
    "\n",
    "* **`CREATE OR REPLACE MODEL`**: This DDL is idempotent. Running it again will update the model if it already exists.\n",
    "* **`REMOTE WITH CONNECTION`**: This tells BigQuery that the model's logic is external and should be accessed via the specified connection.\n",
    "* **`OPTIONS`**: We specify that the remote service is a Cloud AI LLM and provide the model name (`gemini-2.5-pro`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.api_core import exceptions as gax_exceptions\n",
    "\n",
    "model_ready = False\n",
    "if client:\n",
    "    # A writable dataset in your project where the model will be stored.\n",
    "    dataset_id = f\"{PROJECT_ID}.bq_llm_testing\"\n",
    "    dataset = bigquery.Dataset(dataset_id)\n",
    "    dataset.location = LOCATION\n",
    "    try:\n",
    "        client.create_dataset(dataset, exists_ok=True)\n",
    "        print(f\"Dataset ensured: {dataset_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create dataset: {e}\")\n",
    "\n",
    "    # Build the full connection resource string\n",
    "    connection_resource = f\"projects/{PROJECT_ID}/locations/{LOCATION}/connections/{BQ_CONNECTION_ID}\"\n",
    "\n",
    "    # Create (or replace) the remote LLM model in your project\n",
    "    summarizer_query = f\"\"\"\n",
    "    CREATE OR REPLACE MODEL `{dataset_id}.transaction_summarizer`\n",
    "    REMOTE WITH CONNECTION `{connection_resource}`\n",
    "    OPTIONS (\n",
    "      remote_service_type = 'CLOUD_AI_LARGE_LANGUAGE_MODEL_V1',\n",
    "      endpoint = 'gemini-2.5-pro'\n",
    "    );\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        print(\"Creating remote model... (this may take a moment)\")\n",
    "        client.query(summarizer_query).result()\n",
    "        print(f\"Remote LLM model ensured: {dataset_id}.transaction_summarizer\")\n",
    "        model_ready = True\n",
    "    except gax_exceptions.NotFound as e:\n",
    "        print(f\"Connection not found: {connection_resource}\")\n",
    "        print(\"Tip: Ensure you created the BigQuery connection in the correct location with the correct name.\")\n",
    "    except gax_exceptions.Forbidden as e:\n",
    "        print(\"Permission error when creating model.\")\n",
    "        print(\"Tip: Ensure the connection's service account has the 'Vertex AI User' role.\")\n",
    "        print(f\"Details: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create remote model: {e}\")\n",
    "else:\n",
    "    print(\"BigQuery client not available. Skipping model creation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📜 6. Summarize a Transaction with `ML.GENERATE_TEXT`\n",
    "\n",
    "With the model in place, we can now use the `ML.GENERATE_TEXT` function.\n",
    "\n",
    "1.  **Source Data**: We select a single transaction from the public `crypto_ethereum.transactions` table. We specifically look for a transaction that starts with `0x7ff36ab5`, which is the function signature for `swapExactETHForTokens` on Uniswap, making for an interesting summary.\n",
    "2.  **Prompt Engineering**: We create a `prompt` by concatenating our instructions with the raw transaction input data.\n",
    "3.  **`ML.GENERATE_TEXT` call**: We pass our model and the prompt data to the function.\n",
    "4.  **Parameters**: We set `temperature` to a low value for more deterministic output and set a `max_output_tokens` limit.\n",
    "5.  **Result**: The function returns a struct, from which we extract the generated text content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "if model_ready:\n",
    "    # Compose the text-generation query\n",
    "    source_table = \"bigquery-public-data.crypto_ethereum.transactions\"\n",
    "    dataset_id = f\"{PROJECT_ID}.bq_llm_testing\"\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    WITH transactions_to_summarize AS (\n",
    "      SELECT\n",
    "        `hash`,\n",
    "        input AS transaction_input\n",
    "      FROM\n",
    "        `{source_table}`\n",
    "      WHERE STARTS_WITH(input, '0x7ff36ab5') -- Function: swapExactETHForTokens\n",
    "      AND receipt_status = 1 -- Succeeded\n",
    "      LIMIT 1\n",
    "    )\n",
    "    SELECT\n",
    "      t.hash,\n",
    "      --ml_generate_text_result['predictions'][0]['content'] AS summary\n",
    "\n",
    "      ml_generate_text_result as summary,\n",
    "  \t  JSON_EXTRACT_SCALAR(ml_generate_text_result, '$.candidates[0].content.parts[0].text') AS actual_summary_text      \n",
    "      \n",
    "    FROM\n",
    "      ML.GENERATE_TEXT(\n",
    "        MODEL `{dataset_id}.transaction_summarizer`,\n",
    "        (SELECT\n",
    "          CONCAT(\n",
    "            'Explain this Ethereum transaction input data in plain English. ',\n",
    "            'What is the likely function call and what are its parameters? Be concise. Input: ',\n",
    "            transaction_input\n",
    "          ) AS prompt,\n",
    "          `hash`\n",
    "         FROM transactions_to_summarize\n",
    "        ),\n",
    "        STRUCT(\n",
    "          0.2 AS temperature,\n",
    "          5120 AS max_output_tokens\n",
    "        )\n",
    "      ) AS t;\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        print(\"Querying public data and calling remote LLM...\")\n",
    "        df = client.query(query).to_dataframe()\n",
    "\n",
    "        if not df.empty:\n",
    "            tx_hash = df.loc[0, 'hash']\n",
    "            summary = df.loc[0, 'summary']\n",
    "            display(Markdown(f\"### Summary for Transaction: `{tx_hash}`\"))\n",
    "            display(Markdown(summary))\n",
    "        else:\n",
    "            print(\"Query returned no results. Could not find a matching transaction to summarize.\")\n",
    "\n",
    "    except gax_exceptions.Forbidden as e:\n",
    "        print(\"Permission error when calling ML.GENERATE_TEXT.\")\n",
    "        print(\"Tip: Double-check the permissions for the connection's service account.\")\n",
    "        print(f\"Details: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\" Query failed: {e}\")\n",
    "else:\n",
    "    print(\"Skipping text generation because the remote model is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Optional Cleanup\n",
    "\n",
    "Run the following cell to delete the BigQuery model and dataset created during this test. This helps keep your project clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if client:\n",
    "    dataset_id = f\"{PROJECT_ID}.bq_llm_testing\"\n",
    "    model_id = f\"{dataset_id}.transaction_summarizer\"\n",
    "\n",
    "    print(\"Cleaning up resources...\")\n",
    "    \n",
    "    # Drop the model\n",
    "    try:\n",
    "        client.query(f\"DROP MODEL IF EXISTS `{model_id}`\").result()\n",
    "        print(f\"Model dropped: {model_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not drop model: {e}\")\n",
    "\n",
    "    # Drop the dataset (cascade=True deletes contents as well)\n",
    "    try:\n",
    "        client.delete_dataset(dataset_id, delete_contents=True, not_found_ok=True)\n",
    "        print(f\"Dataset dropped: {dataset_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not drop dataset: {e}\")\n",
    "else:\n",
    "    print(\"BigQuery client not available. Skipping cleanup.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "bigq-kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
