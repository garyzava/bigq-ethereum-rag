{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e572efa3",
   "metadata": {},
   "source": [
    "# Test validations\n",
    "Each cell below runs a single test file from the sibling `tests/` folder. Paths are resolved relative to this notebook (no hardcoded absolute paths)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93965428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: /Users/gary/myapps/bigq-kaggle/.venv/bin/python -m pytest -q -rA -s /Users/gary/myapps/bigq-kaggle/tests/test_integration.py\n",
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\n",
      "==================================== PASSES ====================================\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[32mPASSED\u001b[0m tests/test_integration.py::\u001b[1mTestIntegration::test_real_prompt_building\u001b[0m\n",
      "\u001b[32mPASSED\u001b[0m tests/test_integration.py::\u001b[1mTestIntegration::test_real_schema_fetching\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 4.74s\u001b[0m\u001b[0m\n",
      "\u001b[32m.\u001b[0m\n",
      "==================================== PASSES ====================================\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[32mPASSED\u001b[0m tests/test_integration.py::\u001b[1mTestIntegration::test_real_prompt_building\u001b[0m\n",
      "\u001b[32mPASSED\u001b[0m tests/test_integration.py::\u001b[1mTestIntegration::test_real_schema_fetching\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 4.74s\u001b[0m\u001b[0m\n",
      "exit code: 0\n",
      "exit code: 0\n"
     ]
    }
   ],
   "source": [
    "# Run tests/test_integration.py\n",
    "import sys, subprocess, pathlib, shlex\n",
    "nb_dir = pathlib.Path(__file__).parent if '__file__' in globals() else pathlib.Path.cwd()\n",
    "repo_root = nb_dir.parent\n",
    "tests_dir = repo_root / 'tests'\n",
    "file = tests_dir / 'test_integration.py'\n",
    "cmd = [sys.executable, '-m', 'pytest', '-q', '-rA', '-s', str(file)]\n",
    "print('Running:', ' '.join(shlex.quote(x) for x in cmd))\n",
    "res = subprocess.run(cmd, cwd=str(repo_root))\n",
    "print('exit code:', res.returncode)\n",
    "if res.returncode != 0:\n",
    "    raise SystemExit(res.returncode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0155ef77",
   "metadata": {},
   "source": [
    "The following test fails when testing the classifier with some noise. For example adding the word analytics to the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2cb72bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: /Users/gary/myapps/bigq-kaggle/.venv/bin/python -m pytest -q -rA -s /Users/gary/myapps/bigq-kaggle/tests/test_intent_classifier.py\n",
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_______________ test_classify_intent_noise_returns_non_analytics _______________\u001b[0m\n",
      "\n",
      "monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x109557d40>\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_classify_intent_noise_returns_non_analytics\u001b[39;49;00m(monkeypatch):\u001b[90m\u001b[39;49;00m\n",
      "        monkeypatch.setattr(\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mbigq_ai_app.core.intent_classifier.generate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mlambda\u001b[39;49;00m prompt, **kwargs: \u001b[33m\"\u001b[39;49;00m\u001b[33mI think it\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33ms not analytics\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m classify_intent(\u001b[33m\"\u001b[39;49;00m\u001b[33mTell me a joke\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) == \u001b[33m\"\u001b[39;49;00m\u001b[33mnon_analytics\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: assert 'analytics' == 'non_analytics'\u001b[0m\n",
      "\u001b[1m\u001b[31mE         \u001b[0m\n",
      "\u001b[1m\u001b[31mE         \u001b[0m\u001b[91m- non_analytics\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\u001b[0m\n",
      "\u001b[1m\u001b[31mE         ? ----\u001b[90m\u001b[39;49;00m\u001b[0m\n",
      "\u001b[1m\u001b[31mE         \u001b[92m+ analytics\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_intent_classifier.py\u001b[0m:40: AssertionError\n",
      "==================================== PASSES ====================================\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[32mPASSED\u001b[0m tests/test_intent_classifier.py::\u001b[1mtest_classify_intent_analytics\u001b[0m\n",
      "\u001b[32mPASSED\u001b[0m tests/test_intent_classifier.py::\u001b[1mtest_classify_intent_non_analytics\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/test_intent_classifier.py::\u001b[1mtest_classify_intent_noise_returns_non_analytics\u001b[0m - AssertionError: assert 'analytics' == 'non_analytics'\n",
      "\u001b[31m\u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m2 passed\u001b[0m\u001b[31m in 0.51s\u001b[0m\u001b[0m\n",
      "exit code: 1\n",
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_______________ test_classify_intent_noise_returns_non_analytics _______________\u001b[0m\n",
      "\n",
      "monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x109557d40>\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_classify_intent_noise_returns_non_analytics\u001b[39;49;00m(monkeypatch):\u001b[90m\u001b[39;49;00m\n",
      "        monkeypatch.setattr(\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mbigq_ai_app.core.intent_classifier.generate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mlambda\u001b[39;49;00m prompt, **kwargs: \u001b[33m\"\u001b[39;49;00m\u001b[33mI think it\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33ms not analytics\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m classify_intent(\u001b[33m\"\u001b[39;49;00m\u001b[33mTell me a joke\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) == \u001b[33m\"\u001b[39;49;00m\u001b[33mnon_analytics\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: assert 'analytics' == 'non_analytics'\u001b[0m\n",
      "\u001b[1m\u001b[31mE         \u001b[0m\n",
      "\u001b[1m\u001b[31mE         \u001b[0m\u001b[91m- non_analytics\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\u001b[0m\n",
      "\u001b[1m\u001b[31mE         ? ----\u001b[90m\u001b[39;49;00m\u001b[0m\n",
      "\u001b[1m\u001b[31mE         \u001b[92m+ analytics\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_intent_classifier.py\u001b[0m:40: AssertionError\n",
      "==================================== PASSES ====================================\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[32mPASSED\u001b[0m tests/test_intent_classifier.py::\u001b[1mtest_classify_intent_analytics\u001b[0m\n",
      "\u001b[32mPASSED\u001b[0m tests/test_intent_classifier.py::\u001b[1mtest_classify_intent_non_analytics\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/test_intent_classifier.py::\u001b[1mtest_classify_intent_noise_returns_non_analytics\u001b[0m - AssertionError: assert 'analytics' == 'non_analytics'\n",
      "\u001b[31m\u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m2 passed\u001b[0m\u001b[31m in 0.51s\u001b[0m\u001b[0m\n",
      "exit code: 1\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 1\n"
     ]
    }
   ],
   "source": [
    "# Run tests/test_intent_classifier.py\n",
    "import sys, subprocess, pathlib, shlex\n",
    "nb_dir = pathlib.Path(__file__).parent if '__file__' in globals() else pathlib.Path.cwd()\n",
    "repo_root = nb_dir.parent\n",
    "tests_dir = repo_root / 'tests'\n",
    "file = tests_dir / 'test_intent_classifier.py'\n",
    "cmd = [sys.executable, '-m', 'pytest', '-q', '-rA', '-s', str(file)]\n",
    "print('Running:', ' '.join(shlex.quote(x) for x in cmd))\n",
    "res = subprocess.run(cmd, cwd=str(repo_root))\n",
    "print('exit code:', res.returncode)\n",
    "if res.returncode != 0:\n",
    "    raise SystemExit(res.returncode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da24829f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: /Users/gary/myapps/bigq-kaggle/.venv/bin/python -m pytest -q -rA -s /Users/gary/myapps/bigq-kaggle/tests/test_prompt_builder.py\n",
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\n",
      "==================================== PASSES ====================================\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[32mPASSED\u001b[0m tests/test_prompt_builder.py::\u001b[1mTestPromptBuilder::test_build_prompt\u001b[0m\n",
      "\u001b[32mPASSED\u001b[0m tests/test_prompt_builder.py::\u001b[1mTestPromptBuilder::test_get_schema\u001b[0m\n",
      "\u001b[32mPASSED\u001b[0m tests/test_prompt_builder.py::\u001b[1mTestPromptBuilder::test_get_table_schema_error\u001b[0m\n",
      "\u001b[32mPASSED\u001b[0m tests/test_prompt_builder.py::\u001b[1mTestPromptBuilder::test_get_table_schema_success\u001b[0m\n",
      "\u001b[32mPASSED\u001b[0m tests/test_prompt_builder.py::\u001b[1mTestPromptBuilder::test_infer_relationships\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m5 passed\u001b[0m\u001b[32m in 0.74s\u001b[0m\u001b[0m\n",
      "exit code: 0\n",
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\n",
      "==================================== PASSES ====================================\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[32mPASSED\u001b[0m tests/test_prompt_builder.py::\u001b[1mTestPromptBuilder::test_build_prompt\u001b[0m\n",
      "\u001b[32mPASSED\u001b[0m tests/test_prompt_builder.py::\u001b[1mTestPromptBuilder::test_get_schema\u001b[0m\n",
      "\u001b[32mPASSED\u001b[0m tests/test_prompt_builder.py::\u001b[1mTestPromptBuilder::test_get_table_schema_error\u001b[0m\n",
      "\u001b[32mPASSED\u001b[0m tests/test_prompt_builder.py::\u001b[1mTestPromptBuilder::test_get_table_schema_success\u001b[0m\n",
      "\u001b[32mPASSED\u001b[0m tests/test_prompt_builder.py::\u001b[1mTestPromptBuilder::test_infer_relationships\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m5 passed\u001b[0m\u001b[32m in 0.74s\u001b[0m\u001b[0m\n",
      "exit code: 0\n"
     ]
    }
   ],
   "source": [
    "# Run tests/test_prompt_builder.py\n",
    "import sys, subprocess, pathlib, shlex\n",
    "nb_dir = pathlib.Path(__file__).parent if '__file__' in globals() else pathlib.Path.cwd()\n",
    "repo_root = nb_dir.parent\n",
    "tests_dir = repo_root / 'tests'\n",
    "file = tests_dir / 'test_prompt_builder.py'\n",
    "cmd = [sys.executable, '-m', 'pytest', '-q', '-rA', '-s', str(file)]\n",
    "print('Running:', ' '.join(shlex.quote(x) for x in cmd))\n",
    "res = subprocess.run(cmd, cwd=str(repo_root))\n",
    "print('exit code:', res.returncode)\n",
    "if res.returncode != 0:\n",
    "    raise SystemExit(res.returncode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d131de56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: /Users/gary/myapps/bigq-kaggle/.venv/bin/python -m pytest -q -rA -s /Users/gary/myapps/bigq-kaggle/tests/test_sql_generator.py\n",
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\n",
      "==================================== PASSES ====================================\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[32mPASSED\u001b[0m tests/test_sql_generator.py::\u001b[1mtest_qualify_unqualified_name\u001b[0m\n",
      "\u001b[32mPASSED\u001b[0m tests/test_sql_generator.py::\u001b[1mtest_qualify_with_alias\u001b[0m\n",
      "\u001b[32mPASSED\u001b[0m tests/test_sql_generator.py::\u001b[1mtest_not_modify_already_qualified_backticked\u001b[0m\n",
      "\u001b[32mPASSED\u001b[0m tests/test_sql_generator.py::\u001b[1mtest_contains_public_db_true_false\u001b[0m\n",
      "\u001b[32mPASSED\u001b[0m tests/test_sql_generator.py::\u001b[1mtest_extract_sql_from_prompt\u001b[0m\n",
      "\u001b[32mPASSED\u001b[0m tests/test_sql_generator.py::\u001b[1mtest_collapse_duplicated_public_db\u001b[0m\n",
      "\u001b[32mPASSED\u001b[0m tests/test_sql_generator.py::\u001b[1mtest_update_statements_not_modified\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m7 passed\u001b[0m\u001b[32m in 0.69s\u001b[0m\u001b[0m\n",
      "exit code: 0\n",
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\n",
      "==================================== PASSES ====================================\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[32mPASSED\u001b[0m tests/test_sql_generator.py::\u001b[1mtest_qualify_unqualified_name\u001b[0m\n",
      "\u001b[32mPASSED\u001b[0m tests/test_sql_generator.py::\u001b[1mtest_qualify_with_alias\u001b[0m\n",
      "\u001b[32mPASSED\u001b[0m tests/test_sql_generator.py::\u001b[1mtest_not_modify_already_qualified_backticked\u001b[0m\n",
      "\u001b[32mPASSED\u001b[0m tests/test_sql_generator.py::\u001b[1mtest_contains_public_db_true_false\u001b[0m\n",
      "\u001b[32mPASSED\u001b[0m tests/test_sql_generator.py::\u001b[1mtest_extract_sql_from_prompt\u001b[0m\n",
      "\u001b[32mPASSED\u001b[0m tests/test_sql_generator.py::\u001b[1mtest_collapse_duplicated_public_db\u001b[0m\n",
      "\u001b[32mPASSED\u001b[0m tests/test_sql_generator.py::\u001b[1mtest_update_statements_not_modified\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m7 passed\u001b[0m\u001b[32m in 0.69s\u001b[0m\u001b[0m\n",
      "exit code: 0\n"
     ]
    }
   ],
   "source": [
    "# Run tests/test_sql_generator.py\n",
    "import sys, subprocess, pathlib, shlex\n",
    "nb_dir = pathlib.Path(__file__).parent if '__file__' in globals() else pathlib.Path.cwd()\n",
    "repo_root = nb_dir.parent\n",
    "tests_dir = repo_root / 'tests'\n",
    "file = tests_dir / 'test_sql_generator.py'\n",
    "cmd = [sys.executable, '-m', 'pytest', '-q', '-rA', '-s', str(file)]\n",
    "print('Running:', ' '.join(shlex.quote(x) for x in cmd))\n",
    "res = subprocess.run(cmd, cwd=str(repo_root))\n",
    "print('exit code:', res.returncode)\n",
    "if res.returncode != 0:\n",
    "    raise SystemExit(res.returncode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aace26c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: /Users/gary/myapps/bigq-kaggle/.venv/bin/python -m pytest -q -rA -s /Users/gary/myapps/bigq-kaggle/tests/test_utils.py\n",
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\n",
      "==================================== PASSES ====================================\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[32mPASSED\u001b[0m tests/test_utils.py::\u001b[1mtest_choose_table_refs_by_keyword\u001b[0m\n",
      "\u001b[32mPASSED\u001b[0m tests/test_utils.py::\u001b[1mtest_read_schema_catalog_api\u001b[0m\n",
      "\u001b[32mPASSED\u001b[0m tests/test_utils.py::\u001b[1mtest_infer_descriptions_with_ai\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.64s\u001b[0m\u001b[0m\n",
      "exit code: 0\n",
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\n",
      "==================================== PASSES ====================================\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[32mPASSED\u001b[0m tests/test_utils.py::\u001b[1mtest_choose_table_refs_by_keyword\u001b[0m\n",
      "\u001b[32mPASSED\u001b[0m tests/test_utils.py::\u001b[1mtest_read_schema_catalog_api\u001b[0m\n",
      "\u001b[32mPASSED\u001b[0m tests/test_utils.py::\u001b[1mtest_infer_descriptions_with_ai\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.64s\u001b[0m\u001b[0m\n",
      "exit code: 0\n"
     ]
    }
   ],
   "source": [
    "# Run tests/test_utils.py\n",
    "import sys, subprocess, pathlib, shlex\n",
    "nb_dir = pathlib.Path(__file__).parent if '__file__' in globals() else pathlib.Path.cwd()\n",
    "repo_root = nb_dir.parent\n",
    "tests_dir = repo_root / 'tests'\n",
    "file = tests_dir / 'test_utils.py'\n",
    "cmd = [sys.executable, '-m', 'pytest', '-q', '-rA', '-s', str(file)]\n",
    "print('Running:', ' '.join(shlex.quote(x) for x in cmd))\n",
    "res = subprocess.run(cmd, cwd=str(repo_root))\n",
    "print('exit code:', res.returncode)\n",
    "if res.returncode != 0:\n",
    "    raise SystemExit(res.returncode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3dd76851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: /Users/gary/myapps/bigq-kaggle/.venv/bin/python -m pytest -q -rA -s /Users/gary/myapps/bigq-kaggle/tests/test_multimodal_extraction.py\n",
      "\u001b[32m.\u001b[0m\n",
      "==================================== PASSES ====================================\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[32mPASSED\u001b[0m tests/test_multimodal_extraction.py::\u001b[1mtest_extract_text_from_txt\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 1.08s\u001b[0m\u001b[0m\n",
      "exit code: 0\n"
     ]
    }
   ],
   "source": [
    "# Run tests/test_multimodal_extraction.py\n",
    "import sys, subprocess, pathlib, shlex\n",
    "nb_dir = pathlib.Path(__file__).parent if '__file__' in globals() else pathlib.Path.cwd()\n",
    "repo_root = nb_dir.parent\n",
    "tests_dir = repo_root / 'tests'\n",
    "file = tests_dir / 'test_multimodal_extraction.py'\n",
    "cmd = [sys.executable, '-m', 'pytest', '-q', '-rA', '-s', str(file)]\n",
    "print('Running:', ' '.join(shlex.quote(x) for x in cmd))\n",
    "res = subprocess.run(cmd, cwd=str(repo_root))\n",
    "print('exit code:', res.returncode)\n",
    "if res.returncode != 0:\n",
    "    raise SystemExit(res.returncode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigq-kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
